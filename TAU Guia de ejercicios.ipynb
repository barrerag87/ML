{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. ReLU (Rectified Linear Unit)\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "### 2. Sigmoide\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation='sigmoid', input_shape=(1,)),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "### 3. Tanh (Tangente Hiperbólica)\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation='tanh', input_shape=(1,)),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "### 4. Softmax\n",
    "\n",
    "La función Softmax generalmente se utiliza en la última capa de un clasificador para obtener probabilidades de clases.\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
    "  tf.keras.layers.Dense(n_classes, activation='softmax')  # 'n_classes' es el número de clases de salida\n",
    "])\n",
    "```\n",
    "\n",
    "### 5. Softplus\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation='softplus', input_shape=(1,)),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "### 6. LeakyReLU\n",
    "\n",
    "LeakyReLU no está disponible directamente como una cadena en la función de activación, por lo que necesitas agregarla como una capa separada.\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, input_shape=(1,)),\n",
    "  tf.keras.layers.LeakyReLU(alpha=0.01),  # 'alpha' es el valor de pendiente para x < 0\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "### 7. ELU (Exponential Linear Unit)\n",
    "\n",
    "Similar a LeakyReLU, ELU se debe agregar como una capa separada.\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, input_shape=(1,)),\n",
    "  tf.keras.layers.ELU(alpha=1.0),  # 'alpha' controla el valor de saturación para x < 0\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "Al cambiar la función de activación, puedes explorar cómo afecta el comportamiento y el rendimiento de tu modelo de red neuronal. Cada función tiene sus propias características y puede ser más adecuada para ciertos tipos de problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicación de 1\n",
    "\n",
    "La red definida en el código es un modelo secuencial en TensorFlow, lo que significa que las capas se apilan una tras otra en secuencia. Cada capa recibe como entrada la salida de la capa anterior. Este modelo en particular consta de dos capas `Dense` (densas o completamente conectadas):\n",
    "\n",
    "1. **Primera Capa `Dense`**:\n",
    "   - `tf.keras.layers.Dense(10, activation='relu', input_shape=(1,))` define la primera capa densa.\n",
    "   - Tiene 10 neuronas o unidades, lo que significa que hay 10 nodos completamente conectados a la entrada.\n",
    "   - Utiliza la función de activación ReLU (Rectified Linear Unit). ReLU es una función de activación lineal por partes que permite pasar los valores positivos sin cambiarlos, pero convierte los valores negativos en cero. Se usa comúnmente para introducir no linealidades en el modelo, ayudando al modelo a aprender patrones más complejos.\n",
    "   - `input_shape=(1,)` indica que la forma de la entrada es unidimensional, lo que significa que cada instancia de entrada al modelo tendrá una sola característica. Este argumento solo se necesita en la primera capa del modelo secuencial para especificar la forma de la entrada.\n",
    "\n",
    "2. **Segunda Capa `Dense`**:\n",
    "   - `tf.keras.layers.Dense(1)` define la segunda capa densa, que es también la capa de salida del modelo.\n",
    "   - Esta capa tiene una sola neurona o unidad, lo cual es típico para problemas de regresión (donde se predice un valor continuo) o para la capa final en problemas de clasificación binaria (donde se predice una probabilidad).\n",
    "   - No se especifica una función de activación para esta capa en el código dado, lo que implica que es una función de activación lineal por defecto. En el contexto de la regresión, esto significa que la salida de la red se puede interpretar directamente como un valor predicho.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Ejercicio 1: Aproximación de la función exponencial negativa\n",
    "\n",
    "**Objetivo**: Implementar una red neuronal para aproximar la función $ f(x) = e^{-x} $ en el intervalo $[0, 5]$.\n",
    "\n",
    "**Pasos**:\n",
    "1. Generar un conjunto de datos utilizando valores de $ x $ en el intervalo especificado y calculando $ f(x) $ para cada punto.\n",
    "2. Diseñar una red neuronal con al menos una capa oculta. Comenzar con una configuración de 10-20 neuronas y ajustar según sea necesario.\n",
    "3. Utilizar la técnica de regresión para entrenar la red, optimizando el modelo para minimizar la diferencia entre los valores predichos y los reales de $ f(x) $.\n",
    "4. Evaluar el rendimiento del modelo comparando las predicciones de la red con los valores reales de la función.\n",
    "\n",
    "### Ejercicio 2: Aproximación de la función tangente hiperbólica\n",
    "\n",
    "**Objetivo**: Aproximar la función $ f(x) = \\tanh(x) $ utilizando una red neuronal en el intervalo $[-2, 2]$.\n",
    "\n",
    "**Pasos**:\n",
    "1. Crear un conjunto de datos con valores de $ x $ en el intervalo dado y sus correspondientes $ f(x) $.\n",
    "2. Construir una red neuronal con una capa oculta, experimentando con diferentes números de neuronas.\n",
    "3. Entrenar la red usando estos datos, con el objetivo de minimizar el error entre los valores predichos por la red y los valores reales.\n",
    "4. Evaluar la capacidad de la red para replicar la función $ \\tanh(x) $ a través del intervalo especificado.\n",
    "\n",
    "### Ejercicio 3: Aproximación de la función de sinc\n",
    "\n",
    "**Objetivo**: Usar una red neuronal para aproximar la función $ f(x) = \\frac{\\sin(x)}{x} $, tratando $ f(0) = 1 $ para evitar la indeterminación, en el intervalo $[-10, 10]$.\n",
    "\n",
    "**Pasos**:\n",
    "1. Generar un conjunto de datos para $ x $ en el intervalo $[-10, 10]$, usando el valor especial para $ x = 0 $.\n",
    "2. Implementar una red neuronal con una capa oculta y ajustar el número de neuronas para explorar su efecto en la precisión de la aproximación.\n",
    "3. Entrenar la red para minimizar el error entre los valores de $ f(x) $ predichos y los calculados.\n",
    "4. Analizar cómo la red neuronal aproxima la función de sinc, especialmente cerca del origen.\n",
    "\n",
    "### Ejercicio 4: Aproximación de una función polinomial compleja\n",
    "\n",
    "**Objetivo**: Aproximar una función polinomial compleja, por ejemplo, $ f(x) = x^4 - x^3 + x^2 - x $, en el intervalo $[-2, 2]$.\n",
    "\n",
    "**Pasos**:\n",
    "1. Crear un conjunto de datos mapeando valores de $ x $ en el intervalo a sus correspondientes $ f(x) $.\n",
    "2. Diseñar una red neuronal con una o más capas ocultas y comenzar con una cantidad moderada de neuronas en cada capa.\n",
    "3. Entrenar la red para que aprenda a aproximar $ f(x) $ minimizando el error cuadrático medio entre las predicciones de la red y los valores reales.\n",
    "4. Evaluar la efectividad de la red para modelar la función polinomial, prestando atención a cómo maneja las complejidades de sus múltiples términos.\n",
    "\n",
    "Para cada uno de estos ejercicios, es fundamental experimentar con diferentes arquitecturas de red, funciones de activación y hiperparámetros para entender cómo estos factores influyen en la capacidad de la red para aproximar funciones complejas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
